{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61be129e-79d1-42f8-941b-b00ff3ef010a",
   "metadata": {},
   "source": [
    "# Advanced PAHFIT tricks\n",
    "\n",
    "Main goal of this notebook: share some of the tweaks I typically apply to the science pack, the input data, the fitting.\n",
    "You can experiment with these for particularly stubborn fitting problems. \n",
    "\n",
    "Examples include\n",
    "1. Dealing with \"non-finite value\" errors: remove zero uncertainties, prevent very small uncertainties, remove large positive/negative spikes in flux.\n",
    "2. Fits that don't seem to converge: add fudge term to the uncertainty to avoid bad biases, avoid initial guesses near a local minimum\n",
    "3. Faster fitting + additional way to deal with conversion issues: remove gas lines from both spectrum and model, and fit them separately.\n",
    "\n",
    "Some useful code that is not in the standard PAHFIT (but used in this notebook) can be found in pahfit_hacks.py and plot_decomposition.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3860b989-6741-45a6-a732-973cc775ddb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# when ipympl is installed, enable interactive plots like this\n",
    "import matplotlib\n",
    "%matplotlib widget\n",
    "import os\n",
    "from specutils import Spectrum1D\n",
    "from astropy.table import Table \n",
    "from astropy import units as u\n",
    "from astropy.nddata import StdDevUncertainty\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from pahfit.model import Model\n",
    "from specutils import Spectrum1D\n",
    "from plot_decomposition import plot_decomposition\n",
    "from pahfit_hacks import remove_lines_from_data_and_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51efef7f-582c-4348-baeb-913080d1a140",
   "metadata": {},
   "source": [
    "## Load PDRs4All template spectrum data as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7243c6-c537-4263-a158-dada8dac8339",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Table.read('pdrs4all_templates_seg3_crds1084_20230514.dat', \n",
    "               data_start=7, format='ascii', \n",
    "               names=[\"Wave\", \"seg\",  \"flux T1\",\"error T1\", \"flux T2\", \"error T2\", \"flux T3\", \"error T3\", \"flux T4\", \"error T4\", \"flux T5\", \"error T5\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164cd409-9773-440a-a36f-4783772accef",
   "metadata": {},
   "source": [
    "## Cutting the data\n",
    "\n",
    "- Avoid data with bad uncertainties (zero is very bad! fit will crash)\n",
    "- Avoid data with spikes in the flux (speaks for itself. Always clean your data!)\n",
    "- Other problems you might know about in your data. E.g. remove other artifacts such as weird continuum. PAHFIT only has 'physically motivated' continua, so the result can be biased by local continuum-type artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b50cb1-20e2-4e5f-8886-f028d35978ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgood = t[t['flux T3'] > 0]\n",
    "spec = Spectrum1D(tgood['flux T3'] * u.MJy / u.sr, tgood['Wave'] * u.micron, uncertainty=StdDevUncertainty((tgood['error T3'] * u.MJy / u.sr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e82e43b-7070-4566-8f25-f818645575e6",
   "metadata": {},
   "source": [
    "## Double check for unreasonably high signal to noise\n",
    "\n",
    "Typically, if there are points with SNR > 100, one should be worried because those might bias the $\\chi^2$ and get it stuck in a local minimum. In that case, it can be good to add a fudge term that effectively caps the SNR at 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a532680-fd5a-4c34-8c94-1b776d074b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.plot(spec.wavelength, np.abs(spec.flux.value) / spec.uncertainty.array)\n",
    "plt.subplot(122)\n",
    "plt.hist(spec.uncertainty.array)\n",
    "plt.xlabel('uncertainty')\n",
    "print(\"counted zero uncertainties: \", np.count_nonzero(spec.uncertainty.array == 0))\n",
    "print(\"counted SNR > 50: \", np.count_nonzero(spec.uncertainty.array * 50 < spec.flux.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd656a8d-94ce-46a1-97af-eb9fc156be44",
   "metadata": {},
   "source": [
    "## Fit without lines\n",
    "\n",
    "The difference between lines and broad features can be trouble sometimes for the minimizer. As an added bonus, fitting without lines is much faster. Another good idea: use the results of this fit without lines as an initial guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dfb015-69d9-4e80-b9cc-40131020027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "instrumentname = 'jwst.nirspec.g395.high'\n",
    "packfile = \"classic.yaml\"\n",
    "spec.meta['instrument'] = 'jwst.nirspec.g395.high'\n",
    "spec_nolines, model_nolines = remove_lines_from_data_and_model(spec, Model.from_yaml(packfile))\n",
    "model_nolines.guess(spec_nolines)\n",
    "model_nolines.fit(spec_nolines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0babe86a-4cd5-4008-ab0e-3a07a58d287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_decomposition(spec_nolines, model_nolines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed36d30f-b350-4e97-a687-d45a1b55fa21",
   "metadata": {},
   "source": [
    "## Fit with lines using previous result as initial guess\n",
    "\n",
    "First, load a default model (the full one). Then, instead of applying guess(), copy the values from the nolines model instead. Plot this guess to see if it's working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a5660-53fd-4a6a-9621-eb27db10a0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec.meta['instrument'] = instrumentname\n",
    "# redshift = 0.0\n",
    "# spec.set_redshift_to(redshift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aea767-5595-4feb-a7d3-fa92613b5108",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model.from_yaml(packfile)\n",
    "# model.guess(spec)\n",
    "for row in model_nolines.features:\n",
    "    model.features.loc[row['name']] = row\n",
    "\n",
    "_ = plot_decomposition(spec, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b43f84-e990-4afc-ac86-1ce42218263c",
   "metadata": {},
   "source": [
    "Now run the fit. Both the lines and the other features will be adjusted. An even better approach, might be to freeze the dust and continuum features, and only let the gas lines vary. Requires some hacking of the 'power' column in the features table, but is technically possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457fab24-6906-4c1e-8c7c-39052a566f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(spec, maxiter=10000)\n",
    "_ = plot_decomposition(spec, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
